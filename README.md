# NLP-Beginner
Exercises of Natural Language Process.

## Acknowledgements
《[神经网络与深度学习](https://nndl.github.io/)》 

## Task 1: Text categorization based on Softmax Regression
Language: Python

Tool: numpy

Feature extraction: Bag-of-word, N-gram

Data: [Classify the sentiment of sentences from the Rotten Tomatoes dataset](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)

Goal: Implement **Softmax Regression** to categorize different text into different sentiment.

+ ***All code has been uploaded***

Please see my [CSDN blog](https://blog.csdn.net/qq_42365109/article/details/114844020) (in Chinese) for further illustration.

## Task 2: Text categorization based on RNN & CNN
Language: Python

Tool: [pytorch](https://pytorch.org/get-started/locally/), numpy

Feature extraction: Random word embedding, [GloVe](https://nlp.stanford.edu/projects/glove/) word embedding

Data: [Classify the sentiment of sentences from the Rotten Tomatoes dataset](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews)

Goal: Implement Recursive Neural Network **(RNN)** and Convolution Neural Network **(CNN)** to categorize different text into different sentiment.

+ ***All code has been uploaded***

Please see my [CSDN blog](https://blog.csdn.net/qq_42365109/article/details/115140450) (in Chinese) for further illustration.

## Task 3: Text matching based on attention
Language: Python

Tool: [pytorch](https://pytorch.org/get-started/locally/), numpy

Feature extraction: Random word embedding, [GloVe](https://nlp.stanford.edu/projects/glove/) word embedding

Data: [The Stanford Natural Language Inference (SNLI)](https://nlp.stanford.edu/projects/snli/)

Reference：[Enhanced LSTM for Natural Language Inference](https://arxiv.org/pdf/1509.06664v1.pdf)

Goal: Implement ESIM mentioned in the reference above.

+ ***All code has been uploaded***

Please see my [CSDN blog](https://blog.csdn.net/qq_42365109/article/details/115704688) (in Chinese) for further illustration.

## Task 4: Sequence Labeling based on LSTM+CRF
Language: Python

Tool: [pytorch](https://pytorch.org/get-started/locally/), numpy

Feature extraction: Random word embedding, [GloVe](https://nlp.stanford.edu/projects/glove/) word embedding

Data: [CONLL 2003](https://github.com/0oTedo0/NLP-Beginner/blob/main/Project%204%EF%BC%9ASequence%20Labeling:%20LSTM%2BCRF/CoNLL-2003.rar)

Data Introduction: [CONLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/)

Reference：[Neural Architectures for Named Entity Recognition](https://arxiv.org/pdf/1603.01360.pdf)

[End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF](https://arxiv.org/pdf/1603.01354.pdf)

Goal: Sequence Labeling of [CONLL 2003](https://www.clips.uantwerpen.be/conll2003/ner/).

+ ***All code has been uploaded***

Please see my [CSDN blog](https://blog.csdn.net/qq_42365109/article/details/119246515) (in Chinese) for further illustration.

## Task 5: Language Model based on Neural Network 
Language: Python

Tool: [pytorch](https://pytorch.org/get-started/locally/), numpy

Feature extraction: Random word embedding

Data: [poetryFromTang.txt](https://github.com/0oTedo0/NLP-Beginner/blob/main/Project%205%20:%20Language%20Model%20based%20on%20Neural%20Network/poetryFromTang.txt)

Reference：《[神经网络与深度学习 chapter 6, 15](https://nndl.github.io/)》 

Goal: Generating poems.

+ ***All code has been uploaded***

Please see my [CSDN blog](https://blog.csdn.net/qq_42365109/article/details/121921018) (in Chinese) for further illustration.
